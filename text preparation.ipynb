{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statistics import mean\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial import distance\n",
    "from gensim.matutils import softcossim\n",
    "import math\n",
    "import nltk\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "#python -m pip install -U pycld2\n",
    "import pycld2 as cld2\n",
    "import spacy\n",
    "sp = spacy.load(\"en\")\n",
    "sp_stopwords=sp.Defaults.stop_words\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gensim.downloader as api\n",
    "from gensim import corpora,models\n",
    "from gensim.models import WordEmbeddingSimilarityIndex\n",
    "from gensim.similarities import SparseTermSimilarityMatrix\n",
    "w2v_model = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pomoćne funkcije"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Lukre/anaconda3/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def union (list1, list2):\n",
    "    final_list = list(set(list1) | set(list2)) \n",
    "    return final_list \n",
    "\n",
    "def isNaN(string):\n",
    "    return string != string\n",
    "def acronyms_to_words (txt):\n",
    "    #acronyms = pd.read_excel(\"/Users/Lukre/Desktop/D/kratice.xlsx\")\n",
    "    #acronyms = acronyms.rename(columns={'kratice':'acronym','Unnamed: 1':'replacement'})\n",
    "    for i in range (len(acronyms)):\n",
    "        txt=txt.replace(' ' + acronyms.acronym[i] + ' ',' ' + acronyms.replacement[i] + ' ')\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Priprema teksta:  \n",
    "\n",
    "    -obrada i modifikacija u listu tokena\n",
    "    -obrada s povratnom vrijednosti tipa 'str' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Lukre/anaconda3/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def tokenizing_text(txt):\n",
    "    tok=[]\n",
    "    pom=txt.replace('. ',' ')\n",
    "    pom=txt.replace('.',' ')\n",
    "    pom=txt.replace(',',' ')\n",
    "    pom=txt.replace(';',' ')\n",
    "    pom=txt.replace(':',' ')\n",
    "    pom=txt.replace('?',' ')\n",
    "    pom=txt.replace('!',' ')\n",
    "    pom=pom.lower()\n",
    "    pom=acronyms_to_words(pom)\n",
    "    sp_pom=sp(pom)\n",
    "    sp_pom=[x for x in sp_pom if not x.is_stop and x.is_alpha]\n",
    "    for w in sp_pom:\n",
    "        tok.append(w.lemma_)\n",
    "    return tok\n",
    "def prep_text(txt):\n",
    "    txt=txt.replace('. ',' ')\n",
    "    txt=txt.replace('.',' ')\n",
    "    txt=txt.replace(',',' ')\n",
    "    txt=txt.replace(';',' ')\n",
    "    txt=txt.replace(':',' ')\n",
    "    txt=txt.replace('?',' ')\n",
    "    txt=txt.replace('!',' ')\n",
    "    txt=txt.lower()\n",
    "    txt=acronyms_to_words(txt)\n",
    "    sp_pom=sp(txt)\n",
    "    sp_pom=[x for x in sp_pom if not x.is_stop and x.is_alpha]\n",
    "    pom=\"\"\n",
    "    for w in sp_pom:\n",
    "        pom=pom + \" \" + w.lemma_\n",
    "    txt=pom\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obrada svih dokumenata sa prep_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_doc(document,risk,token):\n",
    "    \n",
    "    if (risk=='risk'):\n",
    "        doc = pd.DataFrame(columns=['xx', 'report', 'label','factor', 'accident'])\n",
    "    else:\n",
    "        doc = pd.DataFrame(columns=['xx', 'report'])\n",
    "    \n",
    "    for i in range(len(document)):\n",
    "       \n",
    "        if isNaN(document.report[i]):\n",
    "            continue\n",
    "        isReliable, textBytesFound, details=cld2.detect(document.report[i])\n",
    "        if not isReliable:\n",
    "            continue\n",
    "        if details[0][0]!='ENGLISH':\n",
    "            continue\n",
    "        if (token=='tok'):\n",
    "            txt=tokenizing_text(document.report[i])\n",
    "        else:\n",
    "            txt=prep_text(document.report[i])\n",
    "            \n",
    "        if (risk=='risk'):\n",
    "            if (document.factor[i]==1):\n",
    "                pom='no accident outcome'\n",
    "            elif (document.factor[i] in (2,4,20,100)):\n",
    "                pom='minor injuries or damage'\n",
    "            else:\n",
    "                pom='major or catastrophic accident'\n",
    "            \n",
    "            doc=doc.append({'xx': document.xx[i], 'report': txt, \n",
    "                    'label': document.label[i], 'factor': document.factor[i],\n",
    "                        'accident': pom},ignore_index=True)\n",
    "        \n",
    "        else:\n",
    "            doc=doc.append({'xx': document.xx[i], 'report': txt}, ignore_index=True) \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obrada svih dokumenata i modifikacija u tokene    \n",
    "  \n",
    "  &emsp;fja vraća:  \n",
    "  &emsp;&emsp;-skup svih tokena sa brojem dokumenata u kojima se pojedini token pojavljuje  \n",
    "  &emsp;&emsp;-DataFrame u kojem svaki redak označava jednu kombinaciju dokument-token i tfidf vrijednost te kombinacije"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Lukre/anaconda3/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def tokenizing_Data(doc):\n",
    "    j=0\n",
    "    br=-1\n",
    "    tokens_list=[]\n",
    "    tokens=pd.DataFrame()\n",
    "    TD=pd.DataFrame(columns=['doc','tok','tf_idf'])\n",
    "    n_data=len(doc)\n",
    "    \n",
    "    for i in range (n_data):\n",
    "        \n",
    "        if isNaN(doc.report[i]):\n",
    "            continue\n",
    "        \n",
    "        #provjerava je li izvjestaj na ENG jeziku\n",
    "        isReliable, textBytesFound, details=cld2.detect(doc.report[i])\n",
    "        \n",
    "        if not isReliable:\n",
    "            continue\n",
    "        if details[0][0]!='ENGLISH':\n",
    "            continue\n",
    "            \n",
    "        br=br+1;\n",
    "        tokenized_text=tokenizing_text(doc.report[i])\n",
    "        n_tok=len(tokenized_text)\n",
    "        \n",
    "        if br==0:\n",
    "            tokens_list=union(tokens_list,tokenized_text)\n",
    "            for w in tokens_list:\n",
    "                tokens[w]=0\n",
    "            tokens.loc[0] = 0\n",
    "            for w in tokens_list:\n",
    "                nmb=tokenized_text.count(w)\n",
    "                tokens.loc[[0],[w]] = tokens.loc[[0],[w]] + 1\n",
    "                TD.loc[j]=0\n",
    "                TD.doc[j] = doc.xx[i]\n",
    "                TD.tok[j] = w\n",
    "                TD.tf_idf[j] = nmb/n_tok\n",
    "                j=j+1\n",
    "        else:\n",
    "            pom_tok=union([],tokenized_text)\n",
    "            for w in pom_tok:\n",
    "                nmb=tokenized_text.count(w)\n",
    "                if w not in tokens_list:\n",
    "                    tokens[w] = 0\n",
    "                tokens.loc[[0],[w]] = tokens.loc[[0],[w]] + 1\n",
    "                    \n",
    "                TD.loc[j]=0\n",
    "                TD.doc[j] = doc.xx[i]\n",
    "                TD.tok[j] = w\n",
    "                TD.tf_idf[j] = nmb/n_tok\n",
    "                j=j+1\n",
    "            tokens_list=union(tokens_list,pom_tok)\n",
    "        \n",
    "        if (i%100==0):\n",
    "            print(i)\n",
    "    \n",
    "    N=br+1 \n",
    "    print('j=',j)\n",
    "    for k in range (j):\n",
    "        pom=TD.tf_idf[k]\n",
    "        pom2 = TD.tok[k]\n",
    "        df=pd.to_numeric(tokens[pom2])\n",
    "        l =1+ N/df\n",
    "        l = math.log(l)\n",
    "        TD.tf_idf[k] = pom*l\n",
    "        if (k%1000==0):\n",
    "            print(k)\n",
    "        \n",
    "    return(tokens,TD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fja prima:  \n",
    "&emsp;-DataFrame sa document-token kombinacijama  \n",
    "Fja vraća:  \n",
    "&emsp;-rijetku matricu sa tfidf vrijednostima svih document-token kombinacija"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Lukre/anaconda3/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def pdoc2sparsedf(pdoc):\n",
    "    n=len(set(pdoc.tok))\n",
    "    m=len(set(pdoc.doc))\n",
    "    zero = np.zeros(shape=(m,n))\n",
    "    df=pd.DataFrame(zero,columns=[list(set(pdoc.tok))],index=[list(set(pdoc.doc))])\n",
    "    for i in range (len(pdoc)):\n",
    "        if (i%1000==0):\n",
    "            print(i)\n",
    "        a=[pdoc.doc[i]]\n",
    "        b=[pdoc.tok[i]]\n",
    "        #df.set_value(a, b, pdoc.tf_idf[i])\n",
    "        df.loc[a,b]=pdoc.tf_idf[i]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fja reducira sve tokene koji se pojavljuju u manje od 8 dokumenata  \n",
    "&emsp;Fja vraća:  \n",
    "&emsp;&emsp;-reducirani DataFrame sa svim preostalim document-token kombinacijama i njihovim tfidf vrijednostima  \n",
    "&emsp;&emsp;-DataFrame preostalih tokena sa brojem dokumenata u kojima se pojavljuju  \n",
    "&emsp;&emsp;-listu odbačenih tokena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Lukre/anaconda3/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def pdoc_reduced(pdoc,tokens):\n",
    "    new_pdoc = pd.DataFrame(columns=['doc','tok','tf_idf'])\n",
    "    new_tokens = pd.DataFrame()\n",
    "    ejected_tokens = []\n",
    "    n=len(pdoc)\n",
    "    \n",
    "    for col in tokens.columns:\n",
    "        if (tokens.loc[0][col] < 8):\n",
    "            ejected_tokens.append(col)\n",
    "            continue\n",
    "        else:\n",
    "            new_tokens[col]=0\n",
    "            \n",
    "    new_tokens.loc[0]=0        \n",
    "    for col in new_tokens.columns:\n",
    "        new_tokens.loc[0][col]=tokens.loc[0][col]\n",
    "        \n",
    "    br=0\n",
    "    for i in range(n):\n",
    "        if (i%1000==0):\n",
    "            print(i)\n",
    "            \n",
    "        if (pdoc.tok[i] in new_tokens.columns):\n",
    "            new_pdoc.loc[br]=0\n",
    "            new_pdoc.doc[br]=pdoc.doc[i]\n",
    "            new_pdoc.tok[br]=pdoc.tok[i]\n",
    "            new_pdoc.tf_idf[br]=pdoc.tf_idf[i]\n",
    "            tf=len(pdoc[pdoc.doc==pdoc.doc[i]])\n",
    "            br+=1\n",
    "    print (\"ima ih \", br)\n",
    "   \n",
    "    for i in range(len(new_pdoc)):\n",
    "        if (i%1000==0):\n",
    "            print(i)\n",
    "        tf_idf=new_pdoc.tf_idf[i]\n",
    "        tf_idf=(tf_idf * len(new_pdoc[new_pdoc.doc==new_pdoc.doc[i]])) / len(pdoc[pdoc.doc==pdoc.doc[i]])\n",
    "        new_pdoc.tf_idf[i]=tf_idf\n",
    "        \n",
    "    return new_pdoc, new_tokens , ejected_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Učitavanje dokumenata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Lukre/anaconda3/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "document = pd.read_csv(\"/Users/Lukre/Desktop/D_dipl/qa06_all.csv\")\n",
    "document = document.rename(columns={'qa06id': 'id', 'qa06name':'title', 'qa06wher':'location','qa06dsc':'report'})\n",
    "document_risk = pd.read_csv(\"/Users/Lukre/Desktop/D_dipl/qa06_only_having_risk_valuesl.csv\")\n",
    "document_risk = document_risk.rename(columns={'qa06id': 'id', 'qa06name':'title', 'qa06wher':'location','qa06dsc':'report','ty26colo':'label','ty26fakt':'factor'})\n",
    "\n",
    "acronyms = pd.read_excel(\"/Users/Lukre/Desktop/D_dipl/kratice.xlsx\")\n",
    "acronyms = acronyms.rename(columns={'kratice':'acronym','Unnamed: 1':'replacement'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Priprema dokumenata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Lukre/anaconda3/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "doc_risk_tok=prep_doc(document_risk,'risk','tok')\n",
    "doc_risk_str=prep_doc(document_risk,'risk','str')\n",
    "doc_no_risk_tok=prep_doc(document,'no risk','tok')\n",
    "doc_no_risk_str=prep_doc(document,'no risk','str')\n",
    "\n",
    "pdoc_risk=tokenizing_Data(document_risk)\n",
    "tokens_risk=pdoc_risk[0]\n",
    "pdoc_risk=pdoc_risk[1]\n",
    "\n",
    "pdoc_no_risk=tokenizing_Data(document)\n",
    "tokens_no_risk=pdoc_no_risk[0]\n",
    "pdoc_no_risk=pdoc_no_risk[1]\n",
    "\n",
    "sparse_risk=pdoc2sparsedf(pdoc_risk)\n",
    "sparse_no_risk=pdoc2sparsedf(pdoc_no_risk)\n",
    "\n",
    "pdoc_risk_red=pdoc_reduced(pdoc_risk,tokens_risk)\n",
    "tokens_risk_red=pdoc_risk_red[1]\n",
    "ejected_tokens_risk=pdoc_risk_red[2]\n",
    "pdoc_risk_red=pdoc_risk_red[0]\n",
    "\n",
    "pdoc_no_risk_red=pdoc_reduced(pdoc_no_risk,tokens_no_risk)\n",
    "tokens_no_risk_red=pdoc_no_risk_red[1]\n",
    "ejected_tokens_no_risk=pdoc_no_risk_red[2]\n",
    "pdoc_no_risk_red=pdoc_no_risk_red[0]\n",
    "\n",
    "sparse_risk_red=pdoc2sparsedf(pdoc_risk_red)\n",
    "sparse_no_risk_red=pdoc2sparsedf(pdoc_no_risk_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zapisivanje obrada dokumenata u .csv datoteke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_risk_tok.to_csv('/Users/Lukre/Desktop/D_dipl/doc_risk_tok.csv')\n",
    "doc_risk_str.to_csv('/Users/Lukre/Desktop/D_dipl/doc_risk_str.csv')\n",
    "doc_no_risk_tok.to_csv('/Users/Lukre/Desktop/D_dipl/doc_no_risk_tok.csv')\n",
    "doc_no_risk_str.to_csv('/Users/Lukre/Desktop/D_dipl/doc_no_risk_str.csv')\n",
    "pdoc_risk.to_csv('/Users/Lukre/Desktop/D_dipl/pdoc_risk.csv')\n",
    "tokens_risk.to_csv('/Users/Lukre/Desktop/D_dipl/tokens_risk.csv')\n",
    "pdoc_no_risk.to_csv('/Users/Lukre/Desktop/D_dipl/pdoc_no_risk.csv')\n",
    "tokens_no_risk.to_csv('/Users/Lukre/Desktop/D_dipl/tokens_no_risk.csv')\n",
    "sparse_risk.to_csv('/Users/Lukre/Desktop/D_dipl/sparse_risk.csv')\n",
    "sparse_no_risk.to_csv('/Users/Lukre/Desktop/D_dipl/sparse_no_risk.csv')\n",
    "pdoc_risk_red.to_csv('/Users/Lukre/Desktop/D_dipl/pdoc_risk_red.csv')\n",
    "tokens_risk_red.to_csv('/Users/Lukre/Desktop/D_dipl/tokens_risk_red.csv')\n",
    "pd.DataFrame(ejected_tokens_risk).to_csv('/Users/Lukre/Desktop/D_dipl/ejected_tokens_risk.csv')\n",
    "pdoc_no_risk_red.to_csv('/Users/Lukre/Desktop/D_dipl/pdoc_no_risk_red.csv')\n",
    "tokens_no_risk_red.to_csv('/Users/Lukre/Desktop/D_dipl/tokens_no_risk_red.csv')\n",
    "pd.DataFrame(ejected_tokens_no_risk).to_csv('/Users/Lukre/Desktop/D_dipl/ejected_tokens_no_risk.csv')\n",
    "sparse_risk_red.to_csv('/Users/Lukre/Desktop/D_dipl/sparse_risk_red.csv')\n",
    "sparse_no_risk_red.to_csv('/Users/Lukre/Desktop/D_dipl/sparse_no_risk_red.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
